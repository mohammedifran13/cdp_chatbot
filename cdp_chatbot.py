# -*- coding: utf-8 -*-
"""CDP Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NrjxvKdBipGq5vR3RBBfqQ9ikG9jf-4X

# Scraping URLs

## Scraping Segment, mParticle from thier sitemap

As we see Segment, mParticle has thier URLs listed on sitemap. so we scrap url associated with root /docs

- Segment - https://segment.com/docs/sitemap.xml
- mParticle - https://docs.mparticle.com/sitemap.xml
"""

import requests
import xml.etree.ElementTree as ET
import pandas as pd

# Custom headers
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

def get_urls_from_sitemap(sitemap_url):
    try:
        response = requests.get(sitemap_url, headers=HEADERS, timeout=10)
        response.raise_for_status()  # Raise error if response is not 200

        root = ET.fromstring(response.content)
        namespace = {'ns': "http://www.sitemaps.org/schemas/sitemap/0.9"}

        # Extract URLs from <loc> tags inside <url>
        urls = [elem.text for elem in root.findall(".//ns:url/ns:loc", namespaces=namespace)]

        return urls
    except requests.exceptions.RequestException as e:
        print(f"Error fetching {sitemap_url}: {e}")
        return []

sitemap_urls = {
    "Segment": "https://segment.com/docs/sitemap.xml"
}

doc_urls = {name: get_urls_from_sitemap(url) for name, url in sitemap_urls.items()}

all_urls = [[name, url] for name, urls in doc_urls.items() for url in urls]

df = pd.DataFrame(all_urls, columns=["Source", "URL"])
df.to_csv("Segment.csv", index=False)

import requests
import xml.etree.ElementTree as ET
import pandas as pd

def get_urls_from_sitemap(sitemap_url):
    try:
        response = requests.get(sitemap_url, timeout=10)
        if response.status_code != 200:
            print(f"Failed to fetch {sitemap_url}")
            return []

        root = ET.fromstring(response.text)
        urls = [elem.text for elem in root.findall(".//{http://www.sitemaps.org/schemas/sitemap/0.9}loc")]
        return urls
    except Exception as e:
        print(f"Error processing sitemap {sitemap_url}: {e}")
        return []

# List of sitemap URLs
sitemap_urls = {
    "mParticle": "https://docs.mparticle.com/sitemap.xml"
}

# Scrape all URLs from sitemaps
doc_urls = {}
for name, sitemap_url in sitemap_urls.items():
    print(f"Fetching URLs from {name} sitemap...")
    doc_urls[name] = get_urls_from_sitemap(sitemap_url)

# Save URLs to CSV
all_urls = []
for name, urls in doc_urls.items():
    for url in urls:
        all_urls.append([name, url])

df = pd.DataFrame(all_urls, columns=["Source", "URL"])
df.to_csv("mParticle.csv", index=False)

"""### Scraping for Lytics


We need to scrape URLs from the Lytics Docs site using multithreading. We start from a given URL, extract and follow valid links under /docs/, and store the discovered URLs in a CSV file.
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
from urllib.parse import urljoin, urlparse
from concurrent.futures import ThreadPoolExecutor
import time
from threading import Lock

# Configure session for connection pooling
session = requests.Session()
session.headers.update({
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
})

# Thread-safe sets
discovered_urls = set()
already_visited = set()
lock = Lock()

def fetch_url(url):
    """Fetch and process a single URL"""
    with lock:
        if url in already_visited:
            return []
        already_visited.add(url)

    new_urls = []
    try:
        response = session.get(url, timeout=5)
        if 'text/html' not in response.headers.get('Content-Type', ''):
            return []

        soup = BeautifulSoup(response.text, 'html.parser')

        for link in soup.find_all('a', href=True):
            href = link['href']
            if not href or href.startswith(('javascript:', '#')):
                continue

            absolute_url = urljoin(url, href)
            parsed_url = urlparse(absolute_url)
            clean_url = parsed_url._replace(fragment='').geturl()

            if parsed_url.netloc == 'docs.lytics.com' and parsed_url.path.startswith('/docs/'):
                with lock:
                    if clean_url not in already_visited and clean_url not in discovered_urls:
                        new_urls.append(clean_url)
                        discovered_urls.add(clean_url)

        return new_urls
    except (requests.RequestException, Exception) as e:
        print(f"Error processing {url}: {str(e)}")
        return []

def scrape_website_parallel(start_url, max_workers=5, max_urls=500):
    """Scrape website using multiple threads for efficiency"""
    urls_to_process = [start_url]
    discovered_urls.add(start_url)

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        while urls_to_process and len(discovered_urls) < max_urls:
            batch = urls_to_process[:10]  # Process up to 10 URLs at a time
            urls_to_process = urls_to_process[10:]

            print(f"Processing batch of {len(batch)} URLs. Total discovered: {len(discovered_urls)}")

            future_to_url = {executor.submit(fetch_url, url): url for url in batch}

            for future in future_to_url:
                new_urls = future.result()
                with lock:
                    urls_to_process.extend([url for url in new_urls if url not in already_visited and url not in urls_to_process])

            time.sleep(0.5)  # Delay between batches

    return discovered_urls

if __name__ == "__main__":
    start_time = time.time()

    all_urls = scrape_website_parallel("https://docs.lytics.com/docs/", max_workers=5, max_urls=500)

    url_data = []
    for url in all_urls:
        parsed = urlparse(url)
        path_segments = parsed.path.strip('/').split('/')

        url_data.append({
            "URL": url,
            "Path": parsed.path,
            "Depth": max(len(path_segments) - 1, 0)  # Ensure non-negative depth
        })

    urls_df = pd.DataFrame(url_data).sort_values(by="Path")

    csv_filename = "docs_lytics_urls.csv"
    urls_df.to_csv(csv_filename, index=False)

    try:
        from google.colab import files
        files.download(csv_filename)
    except ImportError:
        pass

    elapsed_time = time.time() - start_time
    print(f"Completed in {elapsed_time:.2f} seconds")
    print(f"Found {len(all_urls)} URLs under docs.lytics.com/docs/")

"""# Scraping Text from URLs"""

import requests
import pandas as pd
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor

def fetch_text(url):
    try:
        response = requests.get(url, timeout=10)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            return " ".join(p.get_text() for p in soup.find_all("p")).strip()
    except Exception as e:
        print(f"Error fetching {url}: {e}")
    return ""

def process_csv(file_path, url_column, output_file):
    df = pd.read_csv(file_path)
    if url_column not in df.columns:
        print(f"Column '{url_column}' not found in {file_path}")
        return

    with ThreadPoolExecutor(max_workers=10) as executor:
        df["Scraped_Text"] = list(executor.map(fetch_text, df[url_column]))

    df.to_csv(output_file, index=False)
    print(f"Scraped data saved to {output_file}")



process_csv("mParticle.csv", "URL", "Mparticle_scraped.csv")
process_csv("docs_lytics_urls.csv", "URL", "docs_lytics_scraped.csv")

# prompt: Add column for /content/docs_lytics_scraped.csv named - Source. All values should be - "Lytics". Delete Path, Depth column

import pandas as pd

# Load the CSV file into a pandas DataFrame
df = pd.read_csv('/content/docs_lytics_scraped.csv')

# Add a new column named 'Source' with the value 'Lytics' for all rows
df['Source'] = 'Lytics'

# Delete the 'Path' and 'Depth' columns
df = df.drop(['Path', 'Depth'], axis=1, errors='ignore')

# Save the modified DataFrame back to the CSV file
df.to_csv('/content/docs_lytics_scraped.csv', index=False)

# prompt: now join /content/docs_lytics_scraped.csv and /content/segment-mparticle_scraped.csv with respect to thier columns. Both csv has same column names. axis = 0

import pandas as pd

# Load the two CSV files into pandas DataFrames
df1 = pd.read_csv('/content/docs_lytics_scraped.csv')
df2 = pd.read_csv('/content/mParticle_scraped.csv')

# Concatenate the DataFrames along the row axis (axis=0)
combined_df = pd.concat([df1, df2], axis=0, ignore_index=True)

# Save the combined DataFrame to a new CSV file
combined_df.to_csv('/content/combined_scraped_data.csv', index=False)

"""# NLP & Information Retrieval"""

!pip install rank_bm25

import nltk
nltk.download('wordnet')
nltk.download('omw-1.4')  # Optional, improves synonym matching

import pandas as pd
import nltk
import spacy
import requests
import random
from google import genai
from nltk.corpus import stopwords, wordnet
from rank_bm25 import BM25Okapi  # Better search ranking
from sentence_transformers import SentenceTransformer, util  # Semantic similarity

# 🔹 Load NLP Models
nltk.download("stopwords")
stop_words = set(stopwords.words("english"))
nlp = spacy.load("en_core_web_sm")
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")  # Better sentence ranking

# 🔹 Load CSV Data
df = pd.read_csv("/content/combined_scraped_data.csv")
df["Processed_Text"] = df["Scraped_Text"].astype(str).str.lower()

# 🔹 Prepare BM25 for search
tokenized_corpus = [doc.split() for doc in df["Processed_Text"]]
bm25 = BM25Okapi(tokenized_corpus)

# 🔹 API Key Switching for Gemini
GEMINI_API_KEYS = [
    "AIzaSyArR5jH7vDbmKR1dyXaRQ3thCZZOqOtE5U",
    "AIzaSyDhzilC3QWrI10xHztZX3mjE3LHZ4qoXI4",
    "AIzaSyCXSSi-499C_ifRNT_ZUCf0l5MOuakX5XM"
]

def get_gemini_client():
    """Randomly selects a Gemini API key to avoid rate limits."""
    api_key = random.choice(GEMINI_API_KEYS)
    return genai.Client(api_key=api_key)

# 🔹 Function: Get Synonyms for Query Expansion
def get_synonyms(word):
    synonyms = set()
    for syn in wordnet.synsets(word):
        for lemma in syn.lemmas():
            synonyms.add(lemma.name().replace("_", " "))
    return list(synonyms)

def expand_query(query):
    words = query.split()
    expanded_words = []
    for word in words:
        expanded_words.extend(get_synonyms(word))
        expanded_words.append(word)  # Keep original word
    return " ".join(expanded_words)

# 🔹 Search Function: Retrieve & Rank Snippets
def search_documents(query, top_n=10):
    expanded_query = expand_query(query)
    query_tokens = expanded_query.split()
    scores = bm25.get_scores(query_tokens)

    # Get top N documents based on BM25 scores
    top_indices = scores.argsort()[-top_n:][::-1]
    retrieved_docs = df.iloc[top_indices][["URL", "Source", "Scraped_Text"]].copy()

    # 🔹 Re-rank using Semantic Similarity
    query_embedding = embedding_model.encode(query, convert_to_tensor=True)
    retrieved_docs["Similarity"] = retrieved_docs["Scraped_Text"].apply(
        lambda x: util.pytorch_cos_sim(query_embedding, embedding_model.encode(x, convert_to_tensor=True)).item()
    )

    # Sort by Semantic Similarity
    retrieved_docs = retrieved_docs.sort_values(by="Similarity", ascending=False)

    # Keep only highly relevant snippets (Similarity > 0.5)
    retrieved_docs = retrieved_docs[retrieved_docs["Similarity"] > 0.5]
    print(retrieved_docs)
    return retrieved_docs

import requests

# 🔹 Extract the Most Relevant Answer Sentence
def extract_answer(text, query):
    doc = nlp(text)
    query_keywords = set(query.lower().split())

    best_sentence = None
    max_match = 0

    for sent in doc.sents:
        words = set(sent.text.lower().split())
        match_count = len(words & query_keywords)

        if match_count > max_match:
            best_sentence = sent.text
            max_match = match_count

    return best_sentence if best_sentence else "⚠️ No relevant sentence found."

# 🔹 First Refinement with Gemini
def refine_with_gemini(query, text, url, retry_count=0):
    if not text or "⚠️" in text:
        return "⚠️ No relevant content to refine."

    client = get_gemini_client()

    prompt = f"""
    You are an **AI support assistant**. Provide a **clear, direct answer** using the relevant snippets.

    ### **Instructions:**
    - **Summarize key points** into a structured, clear answer.
    - **Ensure the answer is concise and includes the source URL.**
    - **Do NOT generate extra content—only summarize what's provided.**

    **Query:** {query}
    **Extracted Answer:** {text}

    **Refined Answer (Source: {url})**:
    """

    try:
        response = client.models.generate_content(model="gemini-2.0-flash", contents=prompt)
        return response.text.strip()
    except Exception as e:
        if retry_count < len(GEMINI_API_KEYS):
            print(f"⚠️ Gemini API failed ({e}). Switching API key...")
            return refine_with_gemini(query, text, url, retry_count + 1)
        return "⚠️ Gemini API unavailable. Unable to refine answer."

# 🔹 Second Refinement for Cleaner Output
def further_refine_with_gemini(query, initial_output, retry_count=0):
    if not initial_output or "⚠️" in initial_output:
        return "⚠️ No meaningful content to refine."

    client = get_gemini_client()

    prompt = f"""
    You are refining an AI-generated response.

    **Instructions:**
    - Remove irrelevant text.
    - Combine key points into a structured, clear answer.
    - Keep only **one** source URL (the most relevant one).

    **Query:** {query}

    **Initial Answer:**
    {initial_output}

    **Final Refined Answer:**
    """

    try:
        response = client.models.generate_content(model="gemini-2.0-flash", contents=prompt)
        return response.text.strip()
    except Exception as e:
        if retry_count < len(GEMINI_API_KEYS):
            print(f"⚠️ Gemini API failed ({e}). Switching API key...")
            return further_refine_with_gemini(query, initial_output, retry_count + 1)
        return "⚠️ Gemini API unavailable. Unable to refine further."

# 🔹 External API Call for Segment & Zeotap Queries
def fetch_external_data(query):
    api_endpoints = [
        "https://phidata-flask.onrender.com/query",
        "https://phidata-flask-9qv9.onrender.com/query"
    ]

    # 🔹 Add extra instructions to the query
    enhanced_query = f"""
    {query}

    **Instructions:**
    - Remove irrelevant text.
    - Combine key points into a structured, clear answer.
    - Keep only **one** source URL (the most relevant one).
    """

    payload = {"question": enhanced_query}
    headers = {"Content-Type": "application/json"}

    for url in api_endpoints:
        try:
            response = requests.post(url, json=payload, headers=headers, timeout=10)
            response.raise_for_status()  # Raise an error for non-2xx responses

            data = response.json().get("response", "")

            # 🔹 Ensure response is unique and remove accidental duplication
            unique_response = "\n".join(set(data.split("\n")))  # Remove duplicate lines
            return unique_response.strip()

        except requests.Timeout:
            print(f"⚠️ Request timed out for {url}. Trying next API...")
        except requests.ConnectionError:
            print(f"⚠️ Network error. Unable to reach {url}. Trying next API...")
        except requests.HTTPError as e:
            print(f"⚠️ API request failed for {url} with status {response.status_code}: {e}. Trying next API...")
        except Exception as e:
            print(f"⚠️ Unexpected error with {url}: {e}. Trying next API...")

    return "⚠️ All API endpoints failed. Unable to retrieve data."


# 🔹 Answer Query with Improved Retrieval & Refinement
def answer_query(query):
    query_lower = query.lower()

    # 🔹 If query contains "Segment" or "Zeotap", use the external API
    if "segment" in query_lower or "zeotap" in query_lower:
        external_answer = fetch_external_data(query)
        print(f"📡 External API Response: {external_answer}")
        return external_answer

    # 🔹 If query contains "Lytics" or "mParticle", use document search
    if "lytics" in query_lower or "mparticle" in query_lower:
        relevant_docs = search_documents(query, top_n=10)
        if relevant_docs.empty:
            return "⚠️ No relevant documents found."

        all_refined_answers = []

        for _, row in relevant_docs.iterrows():
            extracted_answer = extract_answer(row["Scraped_Text"], query)
            refined_answer = refine_with_gemini(query, extracted_answer, row["URL"])
            all_refined_answers.append(refined_answer)

        combined_answer = "\n\n".join(all_refined_answers)

        # 🔹 Run Second Refinement to Clean & Improve Output
        final_answer = further_refine_with_gemini(query, combined_answer)

        return final_answer

    # 🔹 If no CDP is mentioned, ask the user to specify one
    return "⚠️ Specify in which CDP you need to ask the query - Segment, mParticle, Zeotap, or Lytics."

query1 = "How do I set up a new source in Segment?"
answer_query(query1)